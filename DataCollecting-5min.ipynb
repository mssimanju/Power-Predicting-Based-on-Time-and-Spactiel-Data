{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-13 00:31:39,092 - INFO - Start processing data at 2024-12-13 00:31:39.092586\n",
      "2024-12-13 00:31:39,092 - INFO - Processing data from 2023-11 to 2024-4\n",
      "2024-12-13 00:31:39,138 - INFO - Processing data for 2023-11\n",
      "d:\\Anaconda\\envs\\DS5\\lib\\json\\decoder.py:353: RuntimeWarning: coroutine 'main' was never awaited\n",
      "  obj, end = self.scan_once(s, idx)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "2024-12-13 00:31:39,425 - INFO - Month 2023-11 data saved to temp_data\\temp_data_202311.csv\n",
      "2024-12-13 00:31:39,425 - INFO - Processing data for 2023-12\n",
      "2024-12-13 00:31:39,550 - WARNING - Insufficient data points for 2023-12-19: got 421, expected at least 476\n",
      "2024-12-13 00:31:39,550 - ERROR - Data validation failed for 2023-12-19\n",
      "2024-12-13 00:31:39,559 - WARNING - Insufficient data points for 2023-12-20: got 133, expected at least 476\n",
      "2024-12-13 00:31:39,559 - ERROR - Data validation failed for 2023-12-20\n",
      "2024-12-13 00:31:39,559 - WARNING - No data for 2023-12-21\n",
      "2024-12-13 00:31:39,559 - ERROR - Data validation failed for 2023-12-21\n",
      "2024-12-13 00:31:39,567 - WARNING - Insufficient data points for 2023-12-22: got 56, expected at least 476\n",
      "2024-12-13 00:31:39,567 - ERROR - Data validation failed for 2023-12-22\n",
      "2024-12-13 00:31:39,571 - WARNING - Insufficient data points for 2023-12-23: got 332, expected at least 476\n",
      "2024-12-13 00:31:39,571 - ERROR - Data validation failed for 2023-12-23\n",
      "2024-12-13 00:31:39,650 - INFO - Month 2023-12 data saved to temp_data\\temp_data_202312.csv\n",
      "2024-12-13 00:31:39,650 - INFO - Processing data for 2024-1\n",
      "2024-12-13 00:31:39,884 - INFO - Month 2024-1 data saved to temp_data\\temp_data_202401.csv\n",
      "2024-12-13 00:31:39,892 - INFO - Processing data for 2024-2\n",
      "2024-12-13 00:31:40,159 - INFO - Month 2024-2 data saved to temp_data\\temp_data_202402.csv\n",
      "2024-12-13 00:31:40,159 - INFO - Processing data for 2024-3\n",
      "2024-12-13 00:31:40,217 - WARNING - Too many null values for 2024-03-03 in required columns: \n",
      "power              144\n",
      "rainfall           144\n",
      "temperature        144\n",
      "solar_radiation    144\n",
      "dtype: int64\n",
      "2024-12-13 00:31:40,217 - ERROR - Data validation failed for 2024-03-03\n",
      "2024-12-13 00:31:40,217 - WARNING - Too many null values for 2024-03-04 in required columns: \n",
      "power              432\n",
      "rainfall           432\n",
      "temperature        432\n",
      "solar_radiation    432\n",
      "dtype: int64\n",
      "2024-12-13 00:31:40,217 - ERROR - Data validation failed for 2024-03-04\n",
      "2024-12-13 00:31:40,225 - WARNING - Too many null values for 2024-03-05 in required columns: \n",
      "power              576\n",
      "rainfall           576\n",
      "temperature        576\n",
      "solar_radiation    576\n",
      "dtype: int64\n",
      "2024-12-13 00:31:40,225 - ERROR - Data validation failed for 2024-03-05\n",
      "2024-12-13 00:31:40,235 - WARNING - Too many null values for 2024-03-06 in required columns: \n",
      "power              428\n",
      "rainfall           428\n",
      "temperature        428\n",
      "solar_radiation    428\n",
      "dtype: int64\n",
      "2024-12-13 00:31:40,235 - ERROR - Data validation failed for 2024-03-06\n",
      "2024-12-13 00:31:40,235 - WARNING - Insufficient data points for 2024-03-07: got 391, expected at least 476\n",
      "2024-12-13 00:31:40,242 - ERROR - Data validation failed for 2024-03-07\n",
      "2024-12-13 00:31:40,244 - WARNING - Insufficient data points for 2024-03-08: got 103, expected at least 476\n",
      "2024-12-13 00:31:40,244 - ERROR - Data validation failed for 2024-03-08\n",
      "2024-12-13 00:31:40,250 - WARNING - Insufficient data points for 2024-03-09: got 109, expected at least 476\n",
      "2024-12-13 00:31:40,250 - ERROR - Data validation failed for 2024-03-09\n",
      "2024-12-13 00:31:40,253 - WARNING - Insufficient data points for 2024-03-10: got 397, expected at least 476\n",
      "2024-12-13 00:31:40,253 - ERROR - Data validation failed for 2024-03-10\n",
      "2024-12-13 00:31:40,384 - INFO - Month 2024-3 data saved to temp_data\\temp_data_202403.csv\n",
      "2024-12-13 00:31:40,384 - INFO - Processing data for 2024-4\n",
      "2024-12-13 00:31:40,625 - INFO - Month 2024-4 data saved to temp_data\\temp_data_202404.csv\n",
      "2024-12-13 00:31:40,859 - INFO - All data processed and saved to processed_data\\solar_weather_data_5min.csv\n",
      "2024-12-13 00:31:40,859 - INFO - \n",
      "Final data shape: (49323, 5)\n",
      "2024-12-13 00:31:40,859 - INFO - Data points: 49323\n",
      "2024-12-13 00:31:40,859 - INFO - \n",
      "Data statistics:\n",
      "2024-12-13 00:31:40,875 - INFO - \n",
      "              power      rainfall   temperature  solar_radiation\n",
      "count  49182.000000  49205.000000  49205.000000     49205.000000\n",
      "mean       4.608527      0.713497     26.669982       286.051153\n",
      "std        6.126029      2.655923      8.133852       384.593025\n",
      "min       -0.033267      0.000000      0.000000         0.000000\n",
      "25%        0.010000      0.000000     21.182764         1.657658\n",
      "50%        0.120100      0.000000     26.993778        19.594234\n",
      "75%        9.313941      0.000000     33.148983       550.288879\n",
      "max       22.399136     27.800034     45.131031      1503.899536\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 342\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m main()\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 342\u001b[0m     \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\DS5\\lib\\asyncio\\runners.py:33\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(main, debug)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     34\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m coroutines\u001b[38;5;241m.\u001b[39miscoroutine(main):\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma coroutine was expected, got \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(main))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "from typing import Optional, Dict, List\n",
    "from aiohttp import ClientTimeout\n",
    "from asyncio import Semaphore\n",
    "import calendar\n",
    "\n",
    "# ================ Configuration ================\n",
    "# Basic Configuration\n",
    "DATA_DOMAIN = \"https://solarcentre.spinifexvalley.com.au\"\n",
    "LOCATION_WEATHER = \"101\"\n",
    "MAX_CONCURRENT_REQUESTS = 10\n",
    "RETRY_ATTEMPTS = 3\n",
    "REQUEST_TIMEOUT = 30\n",
    "\n",
    "# Cache Configuration\n",
    "CACHE_DIR = \"data_cache\"\n",
    "TEMP_DATA_DIR = \"temp_data\"\n",
    "PROCESSED_DATA_DIR = \"processed_data\"\n",
    "CACHE_EXPIRY_HOURS = 1  # Cache expiration time (hours)\n",
    "\n",
    "# Date Range Configuration\n",
    "START_YEAR = 2023\n",
    "START_MONTH = 11\n",
    "END_YEAR = 2024  # None means current year\n",
    "END_MONTH = 4  # None means current month\n",
    "\n",
    "# Data Source Configuration\n",
    "POWER_SOURCE = '{\"78\":[193]}'\n",
    "WEATHER_SOURCE = '{\"101\":[10021,9000,9304,506]}'\n",
    "\n",
    "# Data Validation Configuration\n",
    "EXPECTED_POINTS_PER_DAY = 576  # Expected data points per day (24 hours * 12 per hour * 2)\n",
    "ALLOWED_MISSING_POINTS = 100    # Maximum allowed missing points\n",
    "TIME_INTERVAL_MINUTES = 5      # Expected time interval (minutes)\n",
    "\n",
    "# HTTP Headers\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "    'Accept': 'application/json, text/javascript, */*; q=0.01',\n",
    "    'Accept-Language': 'en-US,en;q=0.9',\n",
    "    'Accept-Encoding': 'gzip, deflate, br',\n",
    "    'Connection': 'keep-alive',\n",
    "    'Origin': 'https://solarcentre.spinifexvalley.com.au',\n",
    "    'Referer': 'https://solarcentre.spinifexvalley.com.au/graphs'\n",
    "}\n",
    "# ================ End of Configuration ================\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('solar_data.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create necessary directories\n",
    "for directory in [CACHE_DIR, TEMP_DATA_DIR, PROCESSED_DATA_DIR]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "class DataCache:\n",
    "    \"\"\"Data cache management class\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_cache_path(date: datetime, data_type: str) -> str:\n",
    "        \"\"\"Get cache file path\"\"\"\n",
    "        return os.path.join(CACHE_DIR, f\"{data_type}_{date.strftime('%Y%m%d')}.json\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_cache(data: dict, cache_path: str) -> None:\n",
    "        \"\"\"Save data to cache\"\"\"\n",
    "        with open(cache_path, 'w') as f:\n",
    "            json.dump(data, f)\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_cache(cache_path: str) -> Optional[dict]:\n",
    "        \"\"\"Load data from cache\"\"\"\n",
    "        if os.path.exists(cache_path):\n",
    "            try:\n",
    "                # Check if file is expired\n",
    "                file_time = datetime.fromtimestamp(os.path.getmtime(cache_path))\n",
    "                if (datetime.now() - file_time).total_seconds() > CACHE_EXPIRY_HOURS * 3600:\n",
    "                    os.remove(cache_path)\n",
    "                    return None\n",
    "                    \n",
    "                with open(cache_path, 'r') as f:\n",
    "                    return json.load(f)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error loading cache: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "    @staticmethod\n",
    "    def clean_expired_cache() -> None:\n",
    "        \"\"\"Clean expired cache files\"\"\"\n",
    "        try:\n",
    "            current_time = datetime.now()\n",
    "            for filename in os.listdir(CACHE_DIR):\n",
    "                file_path = os.path.join(CACHE_DIR, filename)\n",
    "                file_time = datetime.fromtimestamp(os.path.getmtime(file_path))\n",
    "                \n",
    "                # Delete file if it's expired\n",
    "                if (current_time - file_time).total_seconds() > CACHE_EXPIRY_HOURS * 3600:\n",
    "                    os.remove(file_path)\n",
    "                    logging.info(f\"Removed expired cache file: {filename}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error cleaning cache: {str(e)}\")\n",
    "\n",
    "class DataValidator:\n",
    "    \"\"\"Data validation class\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def validate_day_data(df: pd.DataFrame, date: datetime) -> bool:\n",
    "        \"\"\"Validate daily data integrity\"\"\"\n",
    "        if df is None or df.empty:\n",
    "            logging.warning(f\"No data for {date.date()}\")\n",
    "            return False\n",
    "            \n",
    "        # Check number of data points\n",
    "        points_count = len(df)\n",
    "        if points_count < EXPECTED_POINTS_PER_DAY - ALLOWED_MISSING_POINTS:\n",
    "            logging.warning(f\"Insufficient data points for {date.date()}: \"\n",
    "                          f\"got {points_count}, expected at least \"\n",
    "                          f\"{EXPECTED_POINTS_PER_DAY - ALLOWED_MISSING_POINTS}\")\n",
    "            return False\n",
    "            \n",
    "        # Check time intervals\n",
    "        time_diffs = df.index.to_series().diff().dt.total_seconds() / 60\n",
    "        invalid_intervals = time_diffs[time_diffs != TIME_INTERVAL_MINUTES].count()\n",
    "        if invalid_intervals > ALLOWED_MISSING_POINTS:\n",
    "            logging.warning(f\"Too many irregular time intervals for {date.date()}: \"\n",
    "                          f\"{invalid_intervals} irregular intervals found\")\n",
    "            return False\n",
    "            \n",
    "        # Check data quality (all columns except wind speed)\n",
    "        required_columns = ['power', 'rainfall', 'temperature', 'solar_radiation']\n",
    "        null_counts = df[required_columns].isnull().sum()\n",
    "        if (null_counts > ALLOWED_MISSING_POINTS).any():\n",
    "            logging.warning(f\"Too many null values for {date.date()} in required columns: \\n{null_counts}\")\n",
    "            return False\n",
    "            \n",
    "        # Check outliers\n",
    "        power_outliers = df['power'][\n",
    "            (df['power'] < -1) |  # Negative values are usually errors\n",
    "            (df['power'] > 50)    # Values over 50 are usually anomalies\n",
    "        ].count()\n",
    "        if power_outliers > ALLOWED_MISSING_POINTS:\n",
    "            logging.warning(f\"Too many power outliers for {date.date()}: {power_outliers}\")\n",
    "            return False\n",
    "            \n",
    "        return True\n",
    "\n",
    "async def fetch_data(session: aiohttp.ClientSession, url: str, params: dict, \n",
    "                    semaphore: Semaphore, cache_path: str) -> Optional[dict]:\n",
    "    \"\"\"Asynchronously fetch data with caching and retry mechanism\"\"\"\n",
    "    \n",
    "    # Try to load from cache\n",
    "    cached_data = DataCache.load_cache(cache_path)\n",
    "    if cached_data:\n",
    "        return cached_data\n",
    "        \n",
    "    for attempt in range(RETRY_ATTEMPTS):\n",
    "        try:\n",
    "            async with semaphore:\n",
    "                async with session.get(url, params=params, headers=HEADERS, \n",
    "                                     timeout=ClientTimeout(total=REQUEST_TIMEOUT)) as response:\n",
    "                    if response.status == 200:\n",
    "                        data = await response.json()\n",
    "                        if data.get('message', '').startswith('0'):\n",
    "                            data['message'] = ''\n",
    "                        # Save to cache\n",
    "                        DataCache.save_cache(data, cache_path)\n",
    "                        return data\n",
    "                    logging.error(f\"Request failed with status {response.status}\")\n",
    "        except Exception as e:\n",
    "            if attempt < RETRY_ATTEMPTS - 1:\n",
    "                await asyncio.sleep(2 ** attempt)\n",
    "                continue\n",
    "            logging.error(f\"Error after {RETRY_ATTEMPTS} attempts: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "async def get_day_data(session: aiohttp.ClientSession, date: datetime, \n",
    "                      semaphore: Semaphore) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"Asynchronously fetch daily data\"\"\"\n",
    "    settings = {\n",
    "        'interval': '',\n",
    "        'start_date': date.strftime('%Y-%m-%d'),\n",
    "        'end_date': (date + timedelta(days=1)).strftime('%Y-%m-%d'),\n",
    "        'sources': POWER_SOURCE\n",
    "    }\n",
    "    \n",
    "    weather_settings = {\n",
    "        'interval': '',\n",
    "        'start_date': date.strftime('%Y-%m-%d'),\n",
    "        'end_date': (date + timedelta(days=1)).strftime('%Y-%m-%d'),\n",
    "        'sources': WEATHER_SOURCE\n",
    "    }\n",
    "\n",
    "    power_cache = DataCache.get_cache_path(date, 'power')\n",
    "    weather_cache = DataCache.get_cache_path(date, 'weather')\n",
    "\n",
    "    power_task = fetch_data(session, f\"{DATA_DOMAIN}/power/average\", \n",
    "                           settings, semaphore, power_cache)\n",
    "    weather_task = fetch_data(session, f\"{DATA_DOMAIN}/weather/average\", \n",
    "                            weather_settings, semaphore, weather_cache)\n",
    "    \n",
    "    power_data, weather_data = await asyncio.gather(power_task, weather_task)\n",
    "    \n",
    "    if not all([power_data, weather_data]):\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        if (isinstance(power_data.get('measures'), list) and \n",
    "            isinstance(weather_data.get('measures'), list)):\n",
    "            \n",
    "            power_df = pd.DataFrame(power_data['measures'], \n",
    "                                  columns=['timestamp', 'power'])\n",
    "            power_df['timestamp'] = pd.to_datetime(power_df['timestamp'])\n",
    "            power_df.set_index('timestamp', inplace=True)\n",
    "            \n",
    "            weather_df = pd.DataFrame(weather_data['measures'],\n",
    "                                    columns=['timestamp', 'rainfall', 'temperature', \n",
    "                                           'solar_radiation', 'wind_speed'])\n",
    "            weather_df['timestamp'] = pd.to_datetime(weather_df['timestamp'])\n",
    "            weather_df.set_index('timestamp', inplace=True)\n",
    "            \n",
    "            combined_df = pd.merge(power_df, weather_df, \n",
    "                                 left_index=True, right_index=True, \n",
    "                                 how='outer')\n",
    "            \n",
    "            # Validate data integrity\n",
    "            if DataValidator.validate_day_data(combined_df, date):\n",
    "                return combined_df\n",
    "            else:\n",
    "                logging.error(f\"Data validation failed for {date.date()}\")\n",
    "                return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing data for {date}: {str(e)}\")\n",
    "    return None\n",
    "\n",
    "async def get_month_data(year: int, month: int) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"Asynchronously fetch monthly data\"\"\"\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        all_data = []\n",
    "        _, days_in_month = calendar.monthrange(year, month)\n",
    "        start_date = datetime(year, month, 1)\n",
    "        semaphore = Semaphore(MAX_CONCURRENT_REQUESTS)\n",
    "        \n",
    "        # Create task list\n",
    "        tasks = []\n",
    "        for day in range(days_in_month):\n",
    "            current_date = start_date + timedelta(days=day)\n",
    "            if current_date >= datetime.now():\n",
    "                break\n",
    "            tasks.append(get_day_data(session, current_date, semaphore))\n",
    "\n",
    "        # Run all tasks concurrently\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        all_data = [df for df in results if df is not None]\n",
    "\n",
    "        if all_data:\n",
    "            combined_data = pd.concat(all_data)\n",
    "            combined_data = combined_data[~combined_data.index.duplicated(keep='last')]\n",
    "            combined_data.sort_index(inplace=True)\n",
    "            \n",
    "            # Save temporary file\n",
    "            temp_file = os.path.join(TEMP_DATA_DIR, f\"temp_data_{year}{month:02d}.csv\")\n",
    "            combined_data.to_csv(temp_file)\n",
    "            \n",
    "            logging.info(f\"Month {year}-{month} data saved to {temp_file}\")\n",
    "            return combined_data\n",
    "        return None\n",
    "\n",
    "async def process_all_months(start_year: int = START_YEAR, start_month: int = START_MONTH):\n",
    "    \"\"\"Process all monthly data\"\"\"\n",
    "    current = datetime.now()\n",
    "    end_year = END_YEAR or current.year\n",
    "    end_month = END_MONTH or current.month\n",
    "    all_monthly_data = []\n",
    "    \n",
    "    year, month = start_year, start_month\n",
    "    while True:\n",
    "        # Check if it exceeds the end time\n",
    "        if year > end_year or (year == end_year and month > end_month):\n",
    "            break\n",
    "        # Check if it exceeds the current time\n",
    "        if datetime(year, month, 1) > current:\n",
    "            break\n",
    "            \n",
    "        logging.info(f\"Processing data for {year}-{month}\")\n",
    "        monthly_data = await get_month_data(year, month)\n",
    "        \n",
    "        if monthly_data is not None:\n",
    "            all_monthly_data.append(monthly_data)\n",
    "            \n",
    "        # Update year and month\n",
    "        month += 1\n",
    "        if month > 12:\n",
    "            month = 1\n",
    "            year += 1\n",
    "    \n",
    "    if all_monthly_data:\n",
    "        final_data = pd.concat(all_monthly_data)\n",
    "        final_data = final_data[~final_data.index.duplicated(keep='last')]\n",
    "        final_data.sort_index(inplace=True)\n",
    "        \n",
    "        output_file = os.path.join(PROCESSED_DATA_DIR, 'solar_weather_data_5min.csv')\n",
    "        final_data.to_csv(output_file)\n",
    "        logging.info(f\"All data processed and saved to {output_file}\")\n",
    "        \n",
    "        return final_data\n",
    "    return None\n",
    "\n",
    "async def main():\n",
    "    logging.info(f\"Start processing data at {datetime.now()}\")\n",
    "    logging.info(f\"Processing data from {START_YEAR}-{START_MONTH} to \" + \n",
    "                f\"{END_YEAR or 'current'}-{END_MONTH or 'current'}\")\n",
    "    \n",
    "    # Clean expired cache\n",
    "    DataCache.clean_expired_cache()\n",
    "    \n",
    "    data = await process_all_months()  # Use default values from configuration\n",
    "    if data is not None:\n",
    "        logging.info(f\"\\nFinal data shape: {data.shape}\")\n",
    "        logging.info(f\"Data points: {len(data)}\")\n",
    "        logging.info(\"\\nData statistics:\")\n",
    "        logging.info(f\"\\n{data.describe()}\")\n",
    "        \n",
    "        # Clean expired cache after processing is complete\n",
    "        DataCache.clean_expired_cache()\n",
    "    else:\n",
    "        logging.error(\"Failed to process data\")\n",
    "await main()\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The RuntimeError is beacause of the characteristic of asyncio. The data file can still be generated."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
